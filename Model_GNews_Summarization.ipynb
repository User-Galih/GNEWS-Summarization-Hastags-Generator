{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit nltk beautifulsoup4 requests matplotlib googletrans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj5q6cmbqn0b",
        "outputId": "ac60f717-5c71-48ba-a2a1-cae5ed1dea95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.41.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: googletrans in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans) (0.13.3)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (2025.1.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (3.0.4)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (3.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (1.18.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjPBEOHPqfPP",
        "outputId": "044123f3-fa38-4dc1-ba8c-6f893d5c861a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "2025-01-01 07:04:43.183 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
            "2025-01-01 07:04:43.193 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.445 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-01-01 07:04:43.447 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.452 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.454 Session state does not function when running a script without `streamlit run`\n",
            "2025-01-01 07:04:43.456 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.458 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.461 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.465 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.466 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.469 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.470 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.474 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.475 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.476 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.477 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.478 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.479 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.486 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.487 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.488 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.489 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.491 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.492 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.493 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.494 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.496 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.497 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.498 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.499 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.500 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.501 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-01 07:04:43.502 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import nltk\n",
        "import streamlit as st\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from googletrans import Translator\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Download stopwords for nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define the fixed path to save the Excel file\n",
        "dataset_path = r'C:\\Users\\ThinkPad i7\\OneDrive\\Musik\\dataset stki\\news_summaries.xlsx'\n",
        "\n",
        "# Function to save data to Excel\n",
        "def save_to_excel(dataset):\n",
        "    # Membuat folder jika belum ada\n",
        "    folder_path = os.path.dirname(dataset_path)\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Check if the file exists\n",
        "    if os.path.exists(dataset_path):\n",
        "        # Append data to the existing Excel file\n",
        "        with pd.ExcelWriter(dataset_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
        "            dataset.to_excel(writer, index=False, sheet_name='Summarizations', header=False, startrow=writer.sheets['Summarizations'].max_row)\n",
        "    else:\n",
        "        # Create a new file if it doesn't exist\n",
        "        with pd.ExcelWriter(dataset_path, engine='xlsxwriter') as writer:\n",
        "            dataset.to_excel(writer, index=False, sheet_name='Summarizations')\n",
        "\n",
        "# Function to display dataset\n",
        "@st.cache_data\n",
        "def load_dataset():\n",
        "    # Clear cache before loading the dataset\n",
        "    st.cache_data.clear()\n",
        "\n",
        "    if os.path.exists(dataset_path):\n",
        "        return pd.read_excel(dataset_path, sheet_name='Summarizations')\n",
        "    return pd.DataFrame(columns=['Title', 'Summary', 'Detailed Summary', 'Hashtags', 'Date'])\n",
        "\n",
        "\n",
        "\n",
        "# Initialize stop words for multiple languages\n",
        "stop_words = {\n",
        "    'en': set(stopwords.words('english')).union({'said', 'will', 'also', 'one', 'new', 'make'}),\n",
        "    'id': set(stopwords.words('indonesian')).union({'dan', 'yang', 'di', 'dari', 'pada', 'untuk', 'dengan', 'ke', 'dalam', 'adalah'}),\n",
        "    'es': set(stopwords.words('spanish')).union({'y', 'el', 'en', 'con', 'para', 'de'}),\n",
        "    'fr': set(stopwords.words('french')).union({'et', 'le', 'est', 'dans', 'sur', 'avec'})\n",
        "}\n",
        "\n",
        "# Function to fetch and parse the article from the given URL\n",
        "def fetch_article(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        title = soup.title.string if soup.title else 'No Title Found'\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article = ' '.join([para.get_text() for para in paragraphs])\n",
        "\n",
        "        # Remove common ad phrases\n",
        "        ad_patterns = re.compile(r\"(Advertisement|Scroll to Continue|Baca Juga|Lanjutkan dengan Konten)\", re.IGNORECASE)\n",
        "        article_cleaned = ad_patterns.sub('', article)\n",
        "\n",
        "        return title, article_cleaned.strip()\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "# Function to summarize the article into key points and generate infographic\n",
        "def summarize_article_flexible(article, num_clusters=2):\n",
        "    sentences = sent_tokenize(article)\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "    kmeans = KMeans(n_clusters=num_clusters, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    # Get key points from each cluster\n",
        "    point_summary = []\n",
        "    for i in range(num_clusters):\n",
        "        cluster_sentences = [sentences[j] for j in range(len(sentences)) if kmeans.labels_[j] == i]\n",
        "        if cluster_sentences:\n",
        "            point_summary.append(max(cluster_sentences, key=len))  # Longest sentence as key point\n",
        "\n",
        "    # Short paragraph summary\n",
        "    paragraph_summary = ' '.join(point_summary)\n",
        "\n",
        "    # Infographic: Visualizing word counts\n",
        "    sentence_lengths = [len(sentence.split()) for sentence in point_summary]\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(range(1, len(point_summary) + 1), sentence_lengths, color='skyblue')\n",
        "    plt.xlabel('Point Number')\n",
        "    plt.ylabel('Word Count')\n",
        "    plt.title('Word Count per Key Point')\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    buf.seek(0)\n",
        "\n",
        "    return point_summary, paragraph_summary, buf\n",
        "\n",
        "# Function to generate a longer summary using all sentences\n",
        "def long_summary(article):\n",
        "    sentences = sent_tokenize(article)\n",
        "    return ' '.join(sentences)\n",
        "\n",
        "# Function to translate the article to a specific language\n",
        "def translate_article(article, dest_language='en'):\n",
        "    translator = Translator()\n",
        "    try:\n",
        "        detected_lang = translator.detect(article).lang\n",
        "        if detected_lang != dest_language:\n",
        "            translated = translator.translate(article, dest=dest_language)\n",
        "            return translated.text\n",
        "        else:\n",
        "            return article\n",
        "    except Exception as e:\n",
        "        st.error(f'Translation failed: {e}')\n",
        "        return None\n",
        "\n",
        "# Function to generate hashtags from title and content\n",
        "def generate_hashtags(title, content, lang='en', num_hashtags=5):\n",
        "    stop_words_set = stop_words.get(lang, set())\n",
        "    title_words = [word for word in word_tokenize(title.lower()) if word.isalnum() and len(word) > 3 and word not in stop_words_set]\n",
        "    content_words = [word for word in word_tokenize(content.lower()) if word.isalnum() and len(word) > 3 and word not in stop_words_set]\n",
        "\n",
        "    # Combine title and content words, giving more weight to title words\n",
        "    keywords = title_words * 2 + content_words  # Doubling title words to increase their weight\n",
        "\n",
        "    # Generate TF-IDF scores\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform([' '.join(keywords)])\n",
        "\n",
        "    tfidf_scores = X.toarray().flatten()\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    scored_keywords = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    top_keywords = [f\"#{keyword.capitalize()}\" for keyword, score in scored_keywords[:num_hashtags]]\n",
        "    return top_keywords\n",
        "\n",
        "def main():\n",
        "    st.title('News Summarization & Hashtag Generator App')\n",
        "\n",
        "    # Store URL and selected language in session state\n",
        "    if 'url' not in st.session_state:\n",
        "        st.session_state.url = \"\"\n",
        "    if 'lang' not in st.session_state:\n",
        "        st.session_state.lang = \"en\"\n",
        "\n",
        "    st.session_state.url = st.text_input('Enter the URL of the news article:', st.session_state.url)\n",
        "    st.session_state.lang = st.selectbox('Select language for translation:', ['en', 'id', 'es', 'fr'], index=['en', 'id', 'es', 'fr'].index(st.session_state.lang))\n",
        "\n",
        "    if st.button('Summarize and Generate Hashtags'):\n",
        "        if st.session_state.url:\n",
        "            if not (st.session_state.url.startswith('http://') or st.session_state.url.startswith('https://')):\n",
        "                st.error('Please enter a valid URL starting with http:// or https://')\n",
        "                return\n",
        "\n",
        "            title, article = fetch_article(st.session_state.url)\n",
        "            if article:\n",
        "                if not article.strip():\n",
        "                    st.error('The article is empty or could not be fetched.')\n",
        "                    return\n",
        "\n",
        "                # Translate title if necessary\n",
        "                translated_title = translate_article(title, st.session_state.lang)\n",
        "                st.subheader('Article Title:')\n",
        "                st.write(translated_title)\n",
        "\n",
        "                # Translate the article if necessary\n",
        "                translated_article = translate_article(article, st.session_state.lang)\n",
        "                if translated_article is None:\n",
        "                    return\n",
        "\n",
        "                num_clusters = st.slider('Select the number of clusters for summarization:', 1, 5, 2)\n",
        "                point_summary, paragraph_summary, infographic_buf = summarize_article_flexible(translated_article, num_clusters)\n",
        "\n",
        "                # Display flexible summary options\n",
        "                st.subheader('Flexible Summary Options:')\n",
        "\n",
        "                # Key Points\n",
        "                st.write(\"### Key Points:\")\n",
        "                for idx, point in enumerate(point_summary, 1):\n",
        "                    st.write(f\"{idx}. {point}\")\n",
        "\n",
        "                # Short Paragraph\n",
        "                st.write(\"### Short Paragraph:\")\n",
        "                st.write(paragraph_summary)\n",
        "\n",
        "                # Longer summary\n",
        "                detailed_summary = long_summary(translated_article)\n",
        "                st.write(\"### Detailed Summary:\")\n",
        "                st.write(detailed_summary)\n",
        "\n",
        "\n",
        "                # Generate hashtags\n",
        "                hashtags = generate_hashtags(translated_title, translated_article, st.session_state.lang)\n",
        "                st.subheader('Generated Hashtags:')\n",
        "                st.write(', '.join(hashtags))\n",
        "\n",
        "                st.success(\"Summary and hashtags generated successfully!\")\n",
        "\n",
        "                # Inisialisasi DataFrame kosong\n",
        "                if 'dataset' not in st.session_state:\n",
        "                    st.session_state.dataset = pd.DataFrame(columns=['Title', 'Summary', 'Detailed Summary', 'Hashtags', 'Date'])\n",
        "\n",
        "                # Prepare new data to append\n",
        "                new_data = pd.DataFrame({\n",
        "                    'Title': [translated_title],\n",
        "                    'Summary': [paragraph_summary],\n",
        "                    'Detailed Summary': [detailed_summary],\n",
        "                    'Hashtags': [', '.join(hashtags)],\n",
        "                    'Date': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')]\n",
        "                })\n",
        "\n",
        "                # Save the new data to Excel\n",
        "                save_to_excel(new_data)\n",
        "\n",
        "                st.success('Data added to dataset and saved to Excel!')\n",
        "\n",
        "                # Display dataset in main area\n",
        "                st.subheader('Dataset Viewer')\n",
        "                df = load_dataset()\n",
        "                if not df.empty:\n",
        "                    sort_order = st.selectbox('Sort by Title:', ['Ascending', 'Descending'])\n",
        "                    df = df.sort_values(by='Title', ascending=(sort_order == 'Ascending'))\n",
        "                    st.dataframe(df)\n",
        "                else:\n",
        "                    st.warning('No dataset found. Please generate summaries to create the dataset.')\n",
        "\n",
        "\n",
        "# Run the app\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LEE-H7iQqpyZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}